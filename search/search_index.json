{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DOCUMENTATION","text":""},{"location":"#overview","title":"Overview","text":"<p>This project is an end-to-end ETL (Extract, Transform, Load) streaming data pipeline designed to handle real-time data processing. The pipeline ingests data from a CSV source, processes it through a distributed streaming framework, and stores it in a structured database for analysis and visualization.</p> <p>For a step-by-step deployment guide, visit the Deployment. Happy coding!</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>The pipeline begins by ingesting data from a CSV file into a Kafka topic, enabling real-time event streaming. Spark Streaming then consumes the data, performing necessary transformations and analyses before storing it in PostgreSQL. The processed data is subsequently used to build interactive dashboards in Power BI, allowing for both real-time monitoring and historical data analysis.</p>"},{"location":"#core-components","title":"Core Components","text":"<ul> <li>Kafka: Serves as a message broker, efficiently streaming data from CSV files into Kafka topics.</li> <li>Spark Streaming: Handles real-time data processing and transformation before storing results in PostgreSQL.</li> <li>PostgreSQL: Acts as the structured data storage for processed information, supporting analytical queries.</li> <li>Power BI: Provides interactive dashboards for real-time and historical data visualization.</li> </ul>"},{"location":"#technical-decisions","title":"Technical Decisions","text":"<ul> <li>Kafka: Selected for its capability to handle high-throughput real-time data streaming. Kafka's partitioning mechanism enables parallelism, ensuring scalability and fault tolerance.</li> <li>Spark Streaming: Chosen for its near real-time data processing capabilities with Spark Structured Streaming. By leveraging distributed computing, Spark efficiently processes large-scale streaming data with multiple executors and cores.</li> <li>PostgreSQL: Preferred over MySQL due to its superior analytical capabilities. While MySQL is optimized for high-speed writes, PostgreSQL is better suited for complex queries and analytical workloads.</li> <li>Power BI: Integrated seamlessly with PostgreSQL, making it an ideal choice for intuitive data visualization without requiring extensive additional configurations.</li> </ul>"},{"location":"#technical-notes","title":"Technical Notes","text":""},{"location":"#optimizing-resource-allocation","title":"Optimizing Resource Allocation","text":"<ul> <li>Using a Single Executor: Due to limited CPU and memory resources, a fat executor approach is used\u2014allocating all available resources to a single executor. With 10 cores, each core receives 500MB of memory, optimizing data processing efficiency.</li> <li>Kafka Partitioning Strategy: Kafka topics are configured with 10 partitions to align with Spark's 10 available cores, ensuring parallel processing and maximizing throughput.</li> </ul>"},{"location":"#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Optimizing Joins with Broadcast Join: When joining a large dataset with a smaller one, using broadcast joins eliminates shuffle operations, reducing network overhead and improving query performance.</li> <li>Reducing Spark Shuffle Partitions: The default setting of 200 shuffle partitions in Spark can be excessive for this setup. Reducing it to 9 partitions has significantly improved execution time, reducing task completion from minutes to seconds or even milliseconds.</li> <li>Optimizing Aggregations with Early Filtering: Applying filters before performing aggregations reduces the amount of shuffled data, leading to lower computation time and increased efficiency.</li> </ul>"},{"location":"#checkpointing-in-spark","title":"Checkpointing in Spark","text":"<ul> <li> <p>Checkpointing is used to store the state of the processing pipeline, ensuring fault tolerance and recovery in case of failures (e.g., node failures).</p> </li> <li> <p>Spark Structured Streaming supports checkpointing to persist the state of processing between micro-batches. This enables the system to recover from failures without losing data.</p> </li> <li> <p>Checkpointing ensures the system's resilience and helps recover the stream processing from the point of failure, improving data consistency.</p> </li> </ul>"},{"location":"#expanding-the-pipeline","title":"Expanding the Pipeline","text":"<ul> <li>Scaling Kafka Producers: With 10 partitions in Kafka, data ingestion speed may be affected by network delays or insufficient producers. Increasing the number of producers helps ensure efficient data flow.</li> <li>Enhancing Throughput: Higher throughput can be achieved by increasing the number of Kafka partitions, adding more producers, and allocating additional Spark executors.</li> <li>Ensuring Reliable Message Indexing: Storing message indexes in local files is not ideal for production environments, as failures in message transmission to Kafka could lead to inconsistencies in indexing.</li> <li>Considering OLAP for Scalability: While PostgreSQL works well for small-scale data warehousing, its OLTP nature may limit scalability. Transitioning to an OLAP data warehouse can improve storage efficiency and analytical performance.</li> </ul>"},{"location":"deployment/","title":"New York City Yellow Trip Stream Processing","text":"<p>This reference architecture shows an end-to-end real-time analytics pipeline utilizing ETL (Extract, Transform, Load) processes. The pipeline will ingest data from a source, perform necessary transformations and computations, and load it into storage for future purposes.</p> <p>Scenario: A taxi company that has collected yellow taxi trip data in New York City. The dataset includes several fields such as total_amount, dropoff_location, pickup_location, and trip_distance. There is also a lookup file containing boroughs and zones based on location_id. To analyze trip trends in real-time, the company aims to calculate the number of trips, average revenue per hour,... and detect abnormal trips as early as possible.</p>"},{"location":"deployment/#overview","title":"Overview","text":"<p>The following is the structure of the data pipeline:</p> <p></p>"},{"location":"deployment/#achievements","title":"Achievements","text":"<ul> <li>Detects abnormal trip durations (e.g., less than 1 minute, more than 2 hours) and identifies discrepancies between the actual and calculated amounts.</li> <li>Calculates average revenue and trip counts per hour, categorized by payment type and borough.</li> <li>Configures Spark session with optimized parameters, including custom shuffle partitions and broadcast joins for efficient processing.</li> <li>Provides a dynamic dashboard for monitoring and analyzing trends each day.</li> <li>Efficiently writes large-scale processed streaming data to PostgreSQL in near real-time.</li> </ul> <p>You can download the video in the <code>img/</code> folder to see the dashboard. Or you can visit this link to view it.</p> <p>{: style='width: 100%'}</p>"},{"location":"deployment/#table-of-contents","title":"\ud83d\udcd5 Table Of Contents","text":"<ul> <li>\u2699\ufe0f Local Setup</li> <li>\ud83d\udcbb Deployment<ul> <li>Postgres Setup</li> <li>Kafka Setup</li> <li>Spark Setup</li> </ul> </li> <li>Summary</li> </ul> <p>The sequence to run the script is: <code>create_table.py</code> -&gt; <code>spark_streaming.py</code> -&gt; <code>kafka_stream.py</code></p>"},{"location":"deployment/#local-setup","title":"\u2699\ufe0f Local Setup","text":""},{"location":"deployment/#dataset","title":"\ud83d\udcc2 Dataset","text":"<p>The company has shared with you 3 key datasets for this data.</p> \ud83d\ude95 New York Yellow Taxi Dataset <p>The data file is too large to push it to GitHub. Therefore, please press below to download the zip file and extract it to the <code>data/</code> folder of the project directory. Otherwise, you can place it anywhere you want.</p> <p><li>yellow_tripdata_2024.csv</li></p> Field Description VendorID A code indicating the TPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc. tpep_pickup_datetime The date and time when the meter was engaged. tpep_dropoff_datetime The date and time when the meter was disengaged. Passenger_count The number of passengers in the vehicle. This is a driver-entered value. Trip_distance The elapsed trip distance in miles reported by the taximeter. PULocationID TLC Taxi Zone in which the taximeter was engaged. DOLocationID TLC Taxi Zone in which the taximeter was disengaged. RateCodeID The final rate code in effect at the end of the trip.1= Standard rate 2= JFK 3= Newark 4= Nassau or Westchester 5= Negotiated fare 6= Group ride Store_and_fwd_flag Indicates whether the trip record was held in vehicle memory before sending to the vendor due to no server connection.Y = Store and forward trip N = Not a store and forward trip Payment_type A numeric code signifying how the passenger paid for the trip.1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided trip Fare_amount The time-and-distance fare calculated by the meter. Extra Miscellaneous extras and surcharges, including $0.50 and $1 rush hour and overnight charges. MTA_tax $0.50 MTA tax automatically triggered based on the metered rate in use. Improvement_surcharge $0.30 improvement surcharge assessed at flag drop. This began in 2015. Tip_amount Tip amount. Automatically populated for credit card tips; cash tips are not included. Tolls_amount Total amount of all tolls paid in trip. Total_amount The total amount charged to passengers. Does not include cash tips. Congestion_Surcharge Total amount collected in trip for NYS congestion surcharge. Airport_fee $1.25 for pick up only at LaGuardia and John F. Kennedy Airports. \ud83d\udd0d Taxi Zone Lookup Table <p>This data is used to support the development of dashboard analyses in Power BI.** <li>taxi_zone_lookup</li></p> \ud83d\uddfa\ufe0f Taxi Zone Shapefile <p>This data is used to support the development of dashboard analyses in Power BI. <li>taxi_zone_shapefile</li></p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Docker for running Kafka</li> <li>Install JDK</li> <li>Install Spark</li> <li>Install Python</li> <li>Install PostgreSQL</li> </ul> <p>Apache Spark is only compatible with Java 8, Java 11, or Java 17.</p> <pre><code># Clone the repository\n git clone https://github.com/lnynhi02/Data-Pipeline-Project.git\n</code></pre> <p>Here is the overall structure of the project:</p> <pre><code>Root Directory\n|\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 yellow_tripdata_2024.csv\n\u2502   \u251c\u2500\u2500 taxi_zone_lookup.csv\n\u2502   \u251c\u2500\u2500 taxi_zone_shapefile\n\u2502   \u2514\u2500\u2500 index.txt\n\u2502\n\u251c\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 config.ini\n\u2502\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 create_table.py\n\u2502   \u251c\u2500\u2500 kafka_stream.py\n\u2502   \u2514\u2500\u2500 spark_streaming.py\n\u2502\n\u251c\u2500\u2500 tmp\n\u2502   \u251c\u2500\u2500 sql_warehouse\n\u2502   \u251c\u2500\u2500 local_dir\n\u2502   \u2514\u2500\u2500 checkpoint\n\u2502       \u251c\u2500\u2500 abnormal_duration\n\u2502       \u251c\u2500\u2500 abnormal_fee\n\u2502       \u251c\u2500\u2500 avg_duration\n\u2502       \u251c\u2500\u2500 avg_revenue\n\u2502       \u251c\u2500\u2500 raw_data\n\u2502       \u2514\u2500\u2500 trip_count\n\u2502\n\u2514\u2500\u2500 docker-compose.yaml\n</code></pre> <ul> <li>The <code>config</code> directory contains a <code>config.ini</code> file that includes the configuration of your PostgreSQL database and <code>requirements.txt</code> includes all the essential packages.</li> </ul>"},{"location":"deployment/#deployment","title":"\ud83d\udcbb Deployment","text":""},{"location":"deployment/#postgres-setup","title":"Postgres Setup","text":"<p>Before setting up our Spark and Airflow configurations, let\u2019s create the Postgres database that will persist our data. I prefer using the pgAdmin 4 tool for this; however, any other Postgres development platform can do the job.</p> <p>When installing Postgres, you need to set up a password that we will need later to connect to the database from the Spark environment. You must remember the password to reconnect to the database servers. You can also leave the port at 5432.</p> <p>To create the table and add its columns, we use a script with psycopg2, a PostgreSQL database adapter for Python. We have installed the psycopg2-binary package in the <code>requirements.txt</code> file.</p> <p>Run the Python script with the following command:</p> <pre><code>python src/create_table.py\n</code></pre>"},{"location":"deployment/#kafka-setup","title":"Kafka Setup","text":"<p>To avoid resending messages that have already been processed each time we run the streaming task, we define an <code>index.txt</code> file that records the number of messages sent in the latest streaming session.</p> <p>The code for the Kafka streaming task can be found in <code>src/kafka_stream.py</code>, which involves extracting data from a CSV file, serving the data to a Kafka topic using a Kafka producer, and updating the numbers in <code>index.txt</code>.</p> <p>Run the Kafka service defined in the <code>docker-compose.yaml</code> file:</p> <pre><code>docker-compose up -d\n</code></pre> <p>After the services start, visit the Kafka UI at http://localhost:8800/.</p> <p>Create a topic named yellow_tripdata with:</p> <ul> <li>Replication factor set to 1</li> <li>Partitions set to 10</li> <li>Data retention set to a small number.</li> </ul>"},{"location":"deployment/#spark-setup","title":"Spark Setup","text":"<p>The goal of the Spark jobs is to consume the streaming data from the Kafka topic yellow_tripdata and then transfer it to the Postgres tables.</p> <p>The complete code for the Spark jobs is in the <code>src/spark_streaming.py</code> file.</p> <ul> <li>Create the Spark Session:</li> </ul> <pre><code>def create_sparksession() -&gt; SparkSession:\n    spark = SparkSession.builder \\\n            .appName(\"KafkaToPostgres\") \\\n            .config(\"spark.sql.shuffle.partitions\", \"9\") \\\n            .config(\"spark.sql.warehouse.dir\", \"tmp/sql_warehouse\") \\\n            .config(\"spark.local.dir\", \"tmp/local_dir\") \\\n            .getOrCreate()\n    return spark\n</code></pre> <ul> <li>Read Kafka stream and apply schema:</li> </ul> <pre><code>def create_schema(streaming_df):\n    schema = StructType([\n        StructField('VendorID', StringType(), True),\n        StructField('tpep_pickup_datetime', StringType(), True),\n        StructField('tpep_dropoff_datetime', StringType(), True),\n        StructField('passenger_count', StringType(), True),\n        StructField('trip_distance', StringType(), True),\n        StructField('RatecodeID', StringType(), True),\n        StructField('store_and_fwd_flag', StringType(), True),\n        StructField('PULocationID', StringType(), True),\n        StructField('DOLocationID', StringType(), True),\n        StructField('payment_type', StringType(), True),\n        StructField('fare_amount', StringType(), True),\n        StructField('extra', StringType(), True),\n        StructField('mta_tax', StringType(), True),\n        StructField('tip_amount', StringType(), True),\n        StructField('tolls_amount', StringType(), True),\n        StructField('improvement_surcharge', StringType(), True),\n        StructField('total_amount', StringType(), True),\n        StructField('congestion_surcharge', StringType(), True),\n        StructField('Airport_fee', StringType(), True)\n    ])\n    df = streaming_df.selectExpr(\"CAST(value AS String)\")\\\n        .select(from_json(col('value'), schema).alias('data'))\\\n        .select('data.*')\n    return df\n</code></pre> <ul> <li>Run Spark jobs:</li> </ul> <pre><code>$SPARK_HOME/bin/spark-class.cmd org.apache.spark.deploy.master.Master\n$SPARK_HOME/sbin/start-master.sh\n</code></pre> <ul> <li>Start a Spark worker:</li> </ul> <pre><code>$SPARK_HOME/bin/spark-class.cmd org.apache.spark.deploy.worker.Worker spark://HOST:PORT\n$SPARK_HOME/sbin/start-slave.sh spark://HOST:PORT\n</code></pre> <ul> <li>Submit Spark jobs:</li> </ul> <pre><code>$YOUR_PROJECT_DIRECTORY/spark-submit \\\n--master spark://HOST:PORT \\\n--num-executors 1 \\\n--executor-memory 5G \\\n--total-executor-cores 10 \\\n--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.5.4,org.apache.kafka:kafka-clients:3.7.0 \\\n--driver-class-path /path-to-your-jar/postgresql-x.x.x.jar \\\nsrc/spark_streaming.py\n</code></pre> <p>If there are no errors, you should see successful output in the shell.</p>"},{"location":"deployment/#summary","title":"Summary","text":"<p>Throughout this guide, we\u2019ve thoroughly examined each component of the pipeline, setting up Kafka for data streaming, from processing data with Spark to storing it in PostgreSQL. The incorporation of Docker simplifies the Kafka setup.</p> <p>It's important to note that while this setup is ideal for learning and small-scale projects, scaling it for production use would require additional considerations, particularly regarding security and performance optimization. Future enhancements could include integrating advanced data processing techniques,expanding the pipeline to incorporate more complex data sources.</p> <p>Thank you very much for following along with me. If you have any questions, feel free to inbox me. I hope you enjoy working on the project. Thank you!</p>"}]}